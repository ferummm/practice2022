{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNet2(\n",
      "  (activation): ReLU6(inplace=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bottlenecks): Sequential(\n",
      "    (Bottlenecks_0): Sequential(\n",
      "      (LinearBottleneck0_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_1): Sequential(\n",
      "      (LinearBottleneck1_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck1_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_2): Sequential(\n",
      "      (LinearBottleneck2_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_3): Sequential(\n",
      "      (LinearBottleneck3_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_3): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_4): Sequential(\n",
      "      (LinearBottleneck4_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_5): Sequential(\n",
      "      (LinearBottleneck5_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_6): Sequential(\n",
      "      (LinearBottleneck6_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_last): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn_last): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "MobileNet2(\n",
      "  (activation): ReLU6(inplace=True)\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bottlenecks): Sequential(\n",
      "    (Bottlenecks_0): Sequential(\n",
      "      (LinearBottleneck0_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_1): Sequential(\n",
      "      (LinearBottleneck1_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
      "        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck1_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_2): Sequential(\n",
      "      (LinearBottleneck2_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
      "        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_3): Sequential(\n",
      "      (LinearBottleneck3_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_3): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_4): Sequential(\n",
      "      (LinearBottleneck4_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_5): Sequential(\n",
      "      (LinearBottleneck5_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
      "        (bn2): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
      "        (bn2): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_6): Sequential(\n",
      "      (LinearBottleneck6_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
      "        (bn2): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_last): Conv2d(112, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn_last): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "MobileNet2(\n",
      "  (activation): ReLU6(inplace=True)\n",
      "  (conv1): Conv2d(2, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bottlenecks): Sequential(\n",
      "    (Bottlenecks_0): Sequential(\n",
      "      (LinearBottleneck0_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_1): Sequential(\n",
      "      (LinearBottleneck1_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck1_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_2): Sequential(\n",
      "      (LinearBottleneck2_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_3): Sequential(\n",
      "      (LinearBottleneck3_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_3): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_4): Sequential(\n",
      "      (LinearBottleneck4_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_5): Sequential(\n",
      "      (LinearBottleneck5_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_6): Sequential(\n",
      "      (LinearBottleneck6_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_last): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn_last): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (fc): Linear(in_features=1280, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[-2.2975, -2.3057, -2.3140, -2.3077, -2.3004, -2.3368, -2.3139, -2.2681,\n",
      "         -2.2948, -2.2883]], grad_fn=<LogSoftmaxBackward0>)\n",
      "MobileNet2(\n",
      "  (activation): ReLU6(inplace=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bottlenecks): Sequential(\n",
      "    (Bottlenecks_0): Sequential(\n",
      "      (LinearBottleneck0_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_1): Sequential(\n",
      "      (LinearBottleneck1_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck1_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_2): Sequential(\n",
      "      (LinearBottleneck2_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_3): Sequential(\n",
      "      (LinearBottleneck3_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_3): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_4): Sequential(\n",
      "      (LinearBottleneck4_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_5): Sequential(\n",
      "      (LinearBottleneck5_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_6): Sequential(\n",
      "      (LinearBottleneck6_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_last): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn_last): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (fc): Linear(in_features=1280, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[-2.3028, -2.2910, -2.2692, -2.3370, -2.2995, -2.3111, -2.2910, -2.3046,\n",
      "         -2.3012, -2.3200]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[-2.3281, -2.3088, -2.2778, -2.3143, -2.2896, -2.3028, -2.3170, -2.2786,\n",
      "         -2.3129, -2.2971]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class LinearBottleneck(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1, t=6, activation=nn.ReLU6):\n",
    "        super(LinearBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, inplanes * t, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes * t)\n",
    "        self.conv2 = nn.Conv2d(inplanes * t, inplanes * t, kernel_size=3, stride=stride, padding=1, bias=False,\n",
    "                               groups=inplanes * t)\n",
    "        self.bn2 = nn.BatchNorm2d(inplanes * t)\n",
    "        self.conv3 = nn.Conv2d(inplanes * t, outplanes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(outplanes)\n",
    "        self.activation = activation(inplace=True)\n",
    "        self.stride = stride\n",
    "        self.t = t\n",
    "        self.inplanes = inplanes\n",
    "        self.outplanes = outplanes\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.stride == 1 and self.inplanes == self.outplanes:\n",
    "            out += residual\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet2(nn.Module):\n",
    "    \"\"\"MobileNet2 implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale=1.0, input_size=224, t=6, in_channels=3, num_classes=1000, activation=nn.ReLU6):\n",
    "        \"\"\"\n",
    "        MobileNet2 constructor.\n",
    "        :param in_channels: (int, optional): number of channels in the input tensor.\n",
    "                Default is 3 for RGB image inputs.\n",
    "        :param input_size:\n",
    "        :param num_classes: number of classes to predict. Default\n",
    "                is 1000 for ImageNet.\n",
    "        :param scale:\n",
    "        :param t:\n",
    "        :param activation:\n",
    "        \"\"\"\n",
    "\n",
    "        super(MobileNet2, self).__init__()\n",
    "\n",
    "        self.scale = scale\n",
    "        self.t = t\n",
    "        self.activation_type = activation\n",
    "        self.activation = activation(inplace=True)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_of_channels = [32, 16, 24, 32, 64, 96, 160, 320]\n",
    "        # assert (input_size % 32 == 0)\n",
    "\n",
    "        self.c = [_make_divisible(ch * self.scale, 8) for ch in self.num_of_channels]\n",
    "        self.n = [1, 1, 2, 3, 4, 3, 3, 1]\n",
    "        self.s = [2, 1, 2, 2, 2, 1, 2, 1]\n",
    "        self.conv1 = nn.Conv2d(in_channels, self.c[0], kernel_size=3, bias=False, stride=self.s[0], padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.c[0])\n",
    "        self.bottlenecks = self._make_bottlenecks()\n",
    "\n",
    "        # Last convolution has 1280 output channels for scale <= 1\n",
    "        self.last_conv_out_ch = 1280 if self.scale <= 1 else _make_divisible(1280 * self.scale, 8)\n",
    "        self.conv_last = nn.Conv2d(self.c[-1], self.last_conv_out_ch, kernel_size=1, bias=False)\n",
    "        self.bn_last = nn.BatchNorm2d(self.last_conv_out_ch)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.2, inplace=True)  # confirmed by paper authors\n",
    "        self.fc = nn.Linear(self.last_conv_out_ch, self.num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_stage(self, inplanes, outplanes, n, stride, t, stage):\n",
    "        modules = OrderedDict()\n",
    "        stage_name = \"LinearBottleneck{}\".format(stage)\n",
    "\n",
    "        # First module is the only one utilizing stride\n",
    "        first_module = LinearBottleneck(inplanes=inplanes, outplanes=outplanes, stride=stride, t=t,\n",
    "                                        activation=self.activation_type)\n",
    "        modules[stage_name + \"_0\"] = first_module\n",
    "\n",
    "        # add more LinearBottleneck depending on number of repeats\n",
    "        for i in range(n - 1):\n",
    "            name = stage_name + \"_{}\".format(i + 1)\n",
    "            module = LinearBottleneck(inplanes=outplanes, outplanes=outplanes, stride=1, t=6,\n",
    "                                      activation=self.activation_type)\n",
    "            modules[name] = module\n",
    "\n",
    "        return nn.Sequential(modules)\n",
    "\n",
    "    def _make_bottlenecks(self):\n",
    "        modules = OrderedDict()\n",
    "        stage_name = \"Bottlenecks\"\n",
    "\n",
    "        # First module is the only one with t=1\n",
    "        bottleneck1 = self._make_stage(inplanes=self.c[0], outplanes=self.c[1], n=self.n[1], stride=self.s[1], t=1,\n",
    "                                       stage=0)\n",
    "        modules[stage_name + \"_0\"] = bottleneck1\n",
    "\n",
    "        # add more LinearBottleneck depending on number of repeats\n",
    "        for i in range(1, len(self.c) - 1):\n",
    "            name = stage_name + \"_{}\".format(i)\n",
    "            module = self._make_stage(inplanes=self.c[i], outplanes=self.c[i + 1], n=self.n[i + 1],\n",
    "                                      stride=self.s[i + 1],\n",
    "                                      t=self.t, stage=i)\n",
    "            modules[name] = module\n",
    "\n",
    "        return nn.Sequential(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.bottlenecks(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.bn_last(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # average pooling layer\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # flatten for input to fully-connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1) #TODO not needed(?)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Testing\n",
    "    \"\"\"\n",
    "    model1 = MobileNet2()\n",
    "    print(model1)\n",
    "    model2 = MobileNet2(scale=0.35)\n",
    "    print(model2)\n",
    "    model3 = MobileNet2(in_channels=2, num_classes=10)\n",
    "    print(model3)\n",
    "    x = torch.randn(1, 2, 224, 224)\n",
    "    print(model3(x))\n",
    "    model4_size = 32 * 10\n",
    "    model4 = MobileNet2(input_size=model4_size, num_classes=10)\n",
    "    print(model4)\n",
    "    x2 = torch.randn(1, 3, model4_size, model4_size)\n",
    "    print(model4(x2))\n",
    "    model5 = MobileNet2(input_size=196, num_classes=10)\n",
    "    x3 = torch.randn(1, 3, 196, 196)\n",
    "    print(model5(x3))  # fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class CyclicLR(object):\n",
    "    \"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `batch_step` should be called after a batch has been used for training.\n",
    "    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for eachparam groups.\n",
    "            Default: 0.001\n",
    "        max_lr (float or list): Upper boundaries in the cycle for\n",
    "            each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function. Default: 0.006\n",
    "        step_size (int): Number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch. Default: 2000\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        last_batch_iteration (int): The index of the last batch. Default: -1\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.CyclicLR(optimizer)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>         train_batch(...)\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma ** (x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from clr import CyclicLR\n",
    "\n",
    "\n",
    "def train(model, loader, epoch, optimizer, criterion, device, dtype, batch_size, log_interval, scheduler):\n",
    "    model.train()\n",
    "    correct1, correct5 = 0, 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(loader)):\n",
    "        if isinstance(scheduler, CyclicLR):\n",
    "            scheduler.batch_step()\n",
    "        data, target = data.to(device=device, dtype=dtype), target.to(device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        corr = correct(output, target, topk=(1, 5))\n",
    "        correct1 += corr[0]\n",
    "        correct5 += corr[1]\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            tqdm.write(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}. '\n",
    "                'Top-1 accuracy: {:.2f}%({:.2f}%). '\n",
    "                'Top-5 accuracy: {:.2f}%({:.2f}%).'.format(epoch, batch_idx, len(loader),\n",
    "                                                           100. * batch_idx / len(loader), loss.item(),\n",
    "                                                           100. * corr[0] / batch_size,\n",
    "                                                           100. * correct1 / (batch_size * (batch_idx + 1)),\n",
    "                                                           100. * corr[1] / batch_size,\n",
    "                                                           100. * correct5 / (batch_size * (batch_idx + 1))))\n",
    "    return loss.item(), correct1 / len(loader.dataset), correct5 / len(loader.dataset)\n",
    "\n",
    "\n",
    "def test(model, loader, criterion, device, dtype):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct1, correct5 = 0, 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(loader)):\n",
    "        data, target = data.to(device=device, dtype=dtype), target.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            corr = correct(output, target, topk=(1, 5))\n",
    "        correct1 += corr[0]\n",
    "        correct5 += corr[1]\n",
    "\n",
    "    test_loss /= len(loader)\n",
    "\n",
    "    tqdm.write(\n",
    "        '\\nTest set: Average loss: {:.4f}, Top1: {}/{} ({:.2f}%), '\n",
    "        'Top5: {}/{} ({:.2f}%)'.format(test_loss, int(correct1), len(loader.dataset),\n",
    "                                       100. * correct1 / len(loader.dataset), int(correct5),\n",
    "                                       len(loader.dataset), 100. * correct5 / len(loader.dataset)))\n",
    "    return test_loss, correct1 / len(loader.dataset), correct5 / len(loader.dataset)\n",
    "\n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the correct@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t().type_as(target)\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0).item()\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filepath='./', filename='checkpoint.pth.tar'):\n",
    "    save_path = os.path.join(filepath, filename)\n",
    "    best_path = os.path.join(filepath, 'model_best.pth.tar')\n",
    "    torch.save(state, save_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(save_path, best_path)\n",
    "\n",
    "\n",
    "def find_bounds_clr(model, loader, optimizer, criterion, device, dtype, min_lr=8e-6, max_lr=8e-5, step_size=2000,\n",
    "                    mode='triangular', save_path='.'):\n",
    "    model.train()\n",
    "    correct1, correct5 = 0, 0\n",
    "    scheduler = CyclicLR(optimizer, base_lr=min_lr, max_lr=max_lr, step_size=step_size, mode=mode)\n",
    "    epoch_count = step_size // len(loader)  # Assuming step_size is multiple of batch per epoch\n",
    "    accuracy = []\n",
    "    for _ in trange(epoch_count):\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(loader)):\n",
    "            if scheduler is not None:\n",
    "                scheduler.batch_step()\n",
    "            data, target = data.to(device=device, dtype=dtype), target.to(device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            corr = correct(output, target)\n",
    "            accuracy.append(corr[0] / data.shape[0])\n",
    "\n",
    "    lrs = np.linspace(min_lr, max_lr, step_size)\n",
    "    plt.plot(lrs, accuracy)\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(save_path, 'find_bounds_clr.png'))\n",
    "    np.save(os.path.join(save_path, 'acc.npy'), accuracy)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "__imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
    "                    'std': [0.229, 0.224, 0.225]}\n",
    "\n",
    "\n",
    "def inception_preproccess(input_size, normalize=__imagenet_stats):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**normalize)\n",
    "    ])\n",
    "\n",
    "\n",
    "def scale_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n",
    "    t_list = [\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**normalize),\n",
    "    ]\n",
    "    if scale_size != input_size:\n",
    "        t_list = [transforms.Resize(scale_size)] + t_list\n",
    "\n",
    "    return transforms.Compose(t_list)\n",
    "\n",
    "\n",
    "def get_transform(augment=True, input_size=224):\n",
    "    normalize = __imagenet_stats\n",
    "    scale_size = int(input_size / 0.875)\n",
    "    if augment:\n",
    "        return inception_preproccess(input_size=input_size, normalize=normalize)\n",
    "    else:\n",
    "        return scale_crop(input_size=input_size, scale_size=scale_size, normalize=normalize)\n",
    "\n",
    "\n",
    "def get_loaders(dataroot, val_batch_size, train_batch_size, input_size, workers):\n",
    "    val_data = datasets.ImageFolder(root=os.path.join(dataroot, 'val'), transform=get_transform(False, input_size))\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=val_batch_size, shuffle=False, num_workers=workers,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "    train_data = datasets.ImageFolder(root=os.path.join(dataroot, 'train'),\n",
    "                                      transform=get_transform(input_size=input_size))\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size, shuffle=True,\n",
    "                                               num_workers=workers, pin_memory=True)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class CsvLogger:\n",
    "    def __init__(self, filepath='./', filename='results.csv', data=None):\n",
    "        self.log_path = filepath\n",
    "        self.log_name = filename\n",
    "        self.csv_path = os.path.join(self.log_path, self.log_name)\n",
    "        self.fieldsnames = ['epoch', 'val_error1', 'val_error5', 'val_loss', 'train_error1', 'train_error5',\n",
    "                            'train_loss']\n",
    "\n",
    "        with open(self.csv_path, 'w') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldsnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "        self.data = {}\n",
    "        for field in self.fieldsnames:\n",
    "            self.data[field] = []\n",
    "        if data is not None:\n",
    "            for d in data:\n",
    "                d_num = {}\n",
    "                for key in d:\n",
    "                    d_num[key] = float(d[key]) if key != 'epoch' else int(d[key])\n",
    "                self.write(d_num)\n",
    "\n",
    "    def write(self, data):\n",
    "        for k in self.data:\n",
    "            self.data[k].append(data[k])\n",
    "        with open(self.csv_path, 'a') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldsnames)\n",
    "            writer.writerow(data)\n",
    "\n",
    "    def save_params(self, args, params):\n",
    "        with open(os.path.join(self.log_path, 'params.txt'), 'w') as f:\n",
    "            f.write('{}\\n'.format(' '.join(args)))\n",
    "            f.write('{}\\n'.format(params))\n",
    "\n",
    "    def write_text(self, text, print_t=True):\n",
    "        with open(os.path.join(self.log_path, 'params.txt'), 'a') as f:\n",
    "            f.write('{}\\n'.format(text))\n",
    "        if print_t:\n",
    "            print(text)\n",
    "\n",
    "    def plot_progress_errk(self, claimed_acc=None, title='MobileNetv2', k=1):\n",
    "        tr_str = 'train_error{}'.format(k)\n",
    "        val_str = 'val_error{}'.format(k)\n",
    "        plt.figure(figsize=(9, 8), dpi=300)\n",
    "        plt.plot(self.data[tr_str], label='Training error')\n",
    "        plt.plot(self.data[val_str], label='Validation error')\n",
    "        if claimed_acc is not None:\n",
    "            plt.plot((0, len(self.data[tr_str])), (1 - claimed_acc, 1 - claimed_acc), 'k--',\n",
    "                     label='Claimed validation error ({:.2f}%)'.format(100. * (1 - claimed_acc)))\n",
    "        plt.plot((0, len(self.data[tr_str])),\n",
    "                 (np.min(self.data[val_str]), np.min(self.data[val_str])), 'r--',\n",
    "                 label='Best validation error ({:.2f}%)'.format(100. * np.min(self.data[val_str])))\n",
    "        plt.title('Top-{} error for {}'.format(k, title))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Error')\n",
    "        plt.legend()\n",
    "        plt.xlim(0, len(self.data[tr_str]) + 1)\n",
    "        plt.savefig(os.path.join(self.log_path, 'top{}.png'.format(k)))\n",
    "\n",
    "    def plot_progress_loss(self, title='MobileNetv2'):\n",
    "        plt.figure(figsize=(9, 8), dpi=300)\n",
    "        plt.plot(self.data['train_loss'], label='Training')\n",
    "        plt.plot(self.data['val_loss'], label='Validation')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.xlim(0, len(self.data['train_loss']) + 1)\n",
    "        plt.savefig(os.path.join(self.log_path, 'loss.png'))\n",
    "\n",
    "    def plot_progress(self, claimed_acc1=None, claimed_acc5=None, title='MobileNetv2'):\n",
    "        self.plot_progress_errk(claimed_acc1, title, 1)\n",
    "        self.plot_progress_errk(claimed_acc5, title, 5)\n",
    "        self.plot_progress_loss(title)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flops_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py\n",
    "import torch\n",
    "\n",
    "\n",
    "# ---- Public functions\n",
    "\n",
    "def add_flops_counting_methods(net_main_module):\n",
    "    \"\"\"Adds flops counting functions to an existing model. After that\n",
    "    the flops count should be activated and the model should be run on an input\n",
    "    image.\n",
    "    Example:\n",
    "    fcn = add_flops_counting_methods(fcn)\n",
    "    fcn = fcn.cuda().train()\n",
    "    fcn.start_flops_count()\n",
    "    _ = fcn(batch)\n",
    "    fcn.compute_average_flops_cost() / 1e9 / 2 # Result in GFLOPs per image in batch\n",
    "    Important: dividing by 2 only works for resnet models -- see below for the details\n",
    "    of flops computation.\n",
    "    Attention: we are counting multiply-add as two flops in this work, because in\n",
    "    most resnet models convolutions are bias-free (BN layers act as bias there)\n",
    "    and it makes sense to count muliply and add as separate flops therefore.\n",
    "    This is why in the above example we divide by 2 in order to be consistent with\n",
    "    most modern benchmarks. For example in \"Spatially Adaptive Computatin Time for Residual\n",
    "    Networks\" by Figurnov et al multiply-add was counted as two flops.\n",
    "    This module computes the average flops which is necessary for dynamic networks which\n",
    "    have different number of executed layers. For static networks it is enough to run the network\n",
    "    once and get statistics (above example).\n",
    "    Implementation:\n",
    "    The module works by adding batch_count to the main module which tracks the sum\n",
    "    of all batch sizes that were run through the network.\n",
    "    Also each convolutional layer of the network tracks the overall number of flops\n",
    "    performed.\n",
    "    The parameters are updated with the help of registered hook-functions which\n",
    "    are being called each time the respective layer is executed.\n",
    "    Parameters\n",
    "    ----------\n",
    "    net_main_module : torch.nn.Module\n",
    "        Main module containing network\n",
    "    Returns\n",
    "    -------\n",
    "    net_main_module : torch.nn.Module\n",
    "        Updated main module with new methods/attributes that are used\n",
    "        to compute flops.\n",
    "    \"\"\"\n",
    "\n",
    "    # adding additional methods to the existing module object,\n",
    "    # this is done this way so that each function has access to self object\n",
    "    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n",
    "    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n",
    "    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n",
    "    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n",
    "\n",
    "    net_main_module.reset_flops_count()\n",
    "\n",
    "    # Adding varialbles necessary for masked flops computation\n",
    "    net_main_module.apply(add_flops_mask_variable_or_reset)\n",
    "\n",
    "    return net_main_module\n",
    "\n",
    "\n",
    "def compute_average_flops_cost(self):\n",
    "    \"\"\"\n",
    "    A method that will be available after add_flops_counting_methods() is called\n",
    "    on a desired net object.\n",
    "    Returns current mean flops consumption per image.\n",
    "    \"\"\"\n",
    "\n",
    "    batches_count = self.__batch_counter__\n",
    "\n",
    "    flops_sum = 0\n",
    "\n",
    "    for module in self.modules():\n",
    "\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            flops_sum += module.__flops__\n",
    "\n",
    "    return flops_sum / batches_count\n",
    "\n",
    "\n",
    "def start_flops_count(self):\n",
    "    \"\"\"\n",
    "    A method that will be available after add_flops_counting_methods() is called\n",
    "    on a desired net object.\n",
    "    Activates the computation of mean flops consumption per image.\n",
    "    Call it before you run the network.\n",
    "    \"\"\"\n",
    "\n",
    "    add_batch_counter_hook_function(self)\n",
    "\n",
    "    self.apply(add_flops_counter_hook_function)\n",
    "\n",
    "\n",
    "def stop_flops_count(self):\n",
    "    \"\"\"\n",
    "    A method that will be available after add_flops_counting_methods() is called\n",
    "    on a desired net object.\n",
    "    Stops computing the mean flops consumption per image.\n",
    "    Call whenever you want to pause the computation.\n",
    "    \"\"\"\n",
    "\n",
    "    remove_batch_counter_hook_function(self)\n",
    "\n",
    "    self.apply(remove_flops_counter_hook_function)\n",
    "\n",
    "\n",
    "def reset_flops_count(self):\n",
    "    \"\"\"\n",
    "    A method that will be available after add_flops_counting_methods() is called\n",
    "    on a desired net object.\n",
    "    Resets statistics computed so far.\n",
    "    \"\"\"\n",
    "\n",
    "    add_batch_counter_variables_or_reset(self)\n",
    "\n",
    "    self.apply(add_flops_counter_variable_or_reset)\n",
    "\n",
    "\n",
    "def add_flops_mask(module, mask):\n",
    "    def add_flops_mask_func(module):\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            module.__mask__ = mask\n",
    "\n",
    "    module.apply(add_flops_mask_func)\n",
    "\n",
    "\n",
    "def remove_flops_mask(module):\n",
    "    module.apply(add_flops_mask_variable_or_reset)\n",
    "\n",
    "\n",
    "# ---- Internal functions\n",
    "\n",
    "\n",
    "def conv_flops_counter_hook(conv_module, input, output):\n",
    "    # Can have multiple inputs, getting the first one\n",
    "    input = input[0]\n",
    "\n",
    "    batch_size = input.shape[0]\n",
    "    output_height, output_width = output.shape[2:]\n",
    "\n",
    "    kernel_height, kernel_width = conv_module.kernel_size\n",
    "    in_channels = conv_module.in_channels\n",
    "    out_channels = conv_module.out_channels\n",
    "    groups = conv_module.groups\n",
    "\n",
    "    # We count multiply-add as 2 flops\n",
    "    conv_per_position_flops = 2 * kernel_height * kernel_width * in_channels * out_channels / groups\n",
    "\n",
    "    active_elements_count = batch_size * output_height * output_width\n",
    "\n",
    "    if conv_module.__mask__ is not None:\n",
    "        # (b, 1, h, w)\n",
    "        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n",
    "        active_elements_count = flops_mask.sum()\n",
    "\n",
    "    overall_conv_flops = conv_per_position_flops * active_elements_count\n",
    "\n",
    "    bias_flops = 0\n",
    "\n",
    "    if conv_module.bias is not None:\n",
    "        bias_flops = out_channels * active_elements_count\n",
    "\n",
    "    overall_flops = overall_conv_flops + bias_flops\n",
    "\n",
    "    conv_module.__flops__ += overall_flops\n",
    "\n",
    "\n",
    "def batch_counter_hook(module, input, output):\n",
    "    # Can have multiple inputs, getting the first one\n",
    "    input = input[0]\n",
    "\n",
    "    batch_size = input.shape[0]\n",
    "\n",
    "    module.__batch_counter__ += batch_size\n",
    "\n",
    "\n",
    "def add_batch_counter_variables_or_reset(module):\n",
    "    module.__batch_counter__ = 0\n",
    "\n",
    "\n",
    "def add_batch_counter_hook_function(module):\n",
    "    if hasattr(module, '__batch_counter_handle__'):\n",
    "        return\n",
    "\n",
    "    handle = module.register_forward_hook(batch_counter_hook)\n",
    "    module.__batch_counter_handle__ = handle\n",
    "\n",
    "\n",
    "def remove_batch_counter_hook_function(module):\n",
    "    if hasattr(module, '__batch_counter_handle__'):\n",
    "        module.__batch_counter_handle__.remove()\n",
    "\n",
    "        del module.__batch_counter_handle__\n",
    "\n",
    "\n",
    "def add_flops_counter_variable_or_reset(module):\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        module.__flops__ = 0\n",
    "\n",
    "\n",
    "def add_flops_counter_hook_function(module):\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "        if hasattr(module, '__flops_handle__'):\n",
    "            return\n",
    "\n",
    "        handle = module.register_forward_hook(conv_flops_counter_hook)\n",
    "        module.__flops_handle__ = handle\n",
    "\n",
    "\n",
    "def remove_flops_counter_hook_function(module):\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "        if hasattr(module, '__flops_handle__'):\n",
    "            module.__flops_handle__.remove()\n",
    "\n",
    "            del module.__flops_handle__\n",
    "\n",
    "\n",
    "# --- Masked flops counting\n",
    "\n",
    "\n",
    "# Also being run in the initialization\n",
    "def add_flops_mask_variable_or_reset(module):\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        module.__mask__ = None\n",
    "\n",
    "\n",
    "def count_flops(model, batch_size, device, dtype, input_size, in_channels, *params):\n",
    "    net = model(*params, input_size=input_size)\n",
    "    # print(net)\n",
    "    net = add_flops_counting_methods(net)\n",
    "\n",
    "    net.to(device=device, dtype=dtype)\n",
    "    net = net.train()\n",
    "\n",
    "    batch = torch.randn(batch_size, in_channels, input_size, input_size).to(device=device, dtype=dtype)\n",
    "    net.start_flops_count()\n",
    "\n",
    "    _ = net(batch)\n",
    "    return net.compute_average_flops_cost() / 2  # Result in FLOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, clr=False, dataroot='C:\\\\Users\\\\Admin\\\\Desktop\\\\ТМВ\\\\Практика', decay=4e-05, epochs=400, epochs_per_step=20, evaluate=False, find_clr=False, gamma=0.1, gpus=None, input_size=224, learning_rate=0.01, log_interval=100, max_lr=1, min_lr=1e-05, mode='triangular2', momentum=0.9, results_dir='./results', resume='', save='', scaling=1, schedule=[200, 300], seed=None, start_epoch=0, type='float32', workers=4)\n",
      "Random Seed:  9371\n",
      "MobileNet2(\n",
      "  (activation): ReLU6(inplace=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bottlenecks): Sequential(\n",
      "    (Bottlenecks_0): Sequential(\n",
      "      (LinearBottleneck0_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_1): Sequential(\n",
      "      (LinearBottleneck1_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck1_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_2): Sequential(\n",
      "      (LinearBottleneck2_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck2_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_3): Sequential(\n",
      "      (LinearBottleneck3_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck3_3): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_4): Sequential(\n",
      "      (LinearBottleneck4_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck4_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_5): Sequential(\n",
      "      (LinearBottleneck5_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_1): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "      (LinearBottleneck5_2): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Bottlenecks_6): Sequential(\n",
      "      (LinearBottleneck6_0): LinearBottleneck(\n",
      "        (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_last): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn_last): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (dropout): Dropout(p=0.2, inplace=True)\n",
      "  (fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "number of parameters: 3505960\n",
      "FLOPs: 312339328.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Системе не удается найти указанный путь: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\ТМВ\\\\Практика\\\\val'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-4b6d00badded>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-4b6d00badded>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m                     device, dtype, args.input_size, 3, args.scaling)))\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     train_loader, val_loader = get_loaders(args.dataroot, args.batch_size, args.batch_size, args.input_size,\n\u001b[0m\u001b[0;32m    163\u001b[0m                                            args.workers)\n\u001b[0;32m    164\u001b[0m     \u001b[1;31m# define loss function (criterion) and optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-0a5fe4192e04>\u001b[0m in \u001b[0;36mget_loaders\u001b[1;34m(dataroot, val_batch_size, train_batch_size, input_size, workers)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_loaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     val_loader = torch.utils.data.DataLoader(val_data, batch_size=val_batch_size, shuffle=False, num_workers=workers,\n\u001b[0;32m     46\u001b[0m                                              pin_memory=True)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[0mis_valid_file\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     ):\n\u001b[1;32m--> 310\u001b[1;33m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[0;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[0;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Системе не удается найти указанный путь: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\ТМВ\\\\Практика\\\\val'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm import trange\n",
    "\"\"\"\"\"\n",
    "class Args(dict):\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__\n",
    "\n",
    "    args = {\n",
    "        'dataroot' : None\n",
    "        'gpus' : None\n",
    "        'workers' : 4\n",
    "        'type' : 'float32'\n",
    "        'epochs' : 400\n",
    "        'batch-size' : 64\n",
    "        'learning_rate' : 0.01\n",
    "        'momentum' : 0.9\n",
    "        'decay' : 4e-5\n",
    "        'gamma' : 0.1\n",
    "        'schedule' : [200, 300]\n",
    "        'clr' : 42\n",
    "        'min-lr' : 42\n",
    "        'max-lr' : 42\n",
    "        'epochs-per-step' : 42\n",
    "        'mode' : 42\n",
    "        'find-clr' : 42\n",
    "        'evaluate' : 42\n",
    "        'results_dir' : 42\n",
    "        'resume' : 42\n",
    "        'start-epoch' : 42\n",
    "        'log-interval' : 42\n",
    "        'seed' : 42\n",
    "        'scaling' : 42\n",
    "        'input-size' : 42\n",
    "        \n",
    "    }\n",
    "\n",
    "    args = Args(args) # dict2object\n",
    "    obj = args.copy() # object2dict\n",
    "\"\"\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MobileNetv2 training with PyTorch')\n",
    "parser.add_argument('--dataroot', default=\"C:\\\\Users\\\\Admin\\\\Desktop\\\\ТМВ\\\\Практика\", metavar='PATH',\n",
    "                    help='Path to ImageNet train and val folders, preprocessed as described in '\n",
    "                         'https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset')\n",
    "parser.add_argument('--gpus', default=None, help='List of GPUs used for training - e.g 0,1,3')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='Number of data loading workers (default: 4)')\n",
    "parser.add_argument('--type', default='float32', help='Type of tensor: float32, float16, float64. Default: float32')\n",
    "\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', type=int, default=400, help='Number of epochs to train.')\n",
    "parser.add_argument('-b', '--batch-size', default=64, type=int, metavar='N', help='mini-batch size (default: 64)')\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=0.01, help='The learning rate.')\n",
    "parser.add_argument('--momentum', '-m', type=float, default=0.9, help='Momentum.')\n",
    "parser.add_argument('--decay', '-d', type=float, default=4e-5, help='Weight decay (L2 penalty).')\n",
    "parser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma at scheduled epochs.')\n",
    "parser.add_argument('--schedule', type=int, nargs='+', default=[200, 300],\n",
    "                    help='Decrease learning rate at these epochs.')\n",
    "\n",
    "# CLR\n",
    "parser.add_argument('--clr', dest='clr', action='store_true', help='Use CLR')\n",
    "parser.add_argument('--min-lr', type=float, default=1e-5, help='Minimal LR for CLR.')\n",
    "parser.add_argument('--max-lr', type=float, default=1, help='Maximal LR for CLR.')\n",
    "parser.add_argument('--epochs-per-step', type=int, default=20,\n",
    "                    help='Number of epochs per step in CLR, recommended to be between 2 and 10.')\n",
    "parser.add_argument('--mode', default='triangular2', help='CLR mode. One of {triangular, triangular2, exp_range}')\n",
    "parser.add_argument('--find-clr', dest='find_clr', action='store_true',\n",
    "                    help='Run search for optimal LR in range (min_lr, max_lr)')\n",
    "\n",
    "# Checkpoints\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true', help='Just evaluate model')\n",
    "parser.add_argument('--save', '-s', type=str, default='', help='Folder to save checkpoints.')\n",
    "parser.add_argument('--results_dir', metavar='RESULTS_DIR', default='./results', help='Directory to store results')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N', help='Number of batches between log messages')\n",
    "parser.add_argument('--seed', type=int, default=None, metavar='S', help='random seed (default: 1)')\n",
    "\n",
    "# Architecture\n",
    "parser.add_argument('--scaling', type=float, default=1, metavar='SC', help='Scaling of MobileNet (default x1).')\n",
    "parser.add_argument('--input-size', type=int, default=224, metavar='I',\n",
    "                    help='Input size of MobileNet, multiple of 32 (default 224).')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/fe066966b5afa96f2f6b9f71ec0c71158b44068d/keras/applications/mobilenetv2.py#L30\n",
    "claimed_acc_top1 = {224: {1.4: 0.75, 1.3: 0.744, 1.0: 0.718, 0.75: 0.698, 0.5: 0.654, 0.35: 0.603},\n",
    "                    192: {1.0: 0.707, 0.75: 0.687, 0.5: 0.639, 0.35: 0.582},\n",
    "                    160: {1.0: 0.688, 0.75: 0.664, 0.5: 0.610, 0.35: 0.557},\n",
    "                    128: {1.0: 0.653, 0.75: 0.632, 0.5: 0.577, 0.35: 0.508},\n",
    "                    96: {1.0: 0.603, 0.75: 0.588, 0.5: 0.512, 0.35: 0.455},\n",
    "                    }\n",
    "claimed_acc_top5 = {224: {1.4: 0.925, 1.3: 0.921, 1.0: 0.910, 0.75: 0.896, 0.5: 0.864, 0.35: 0.829},\n",
    "                    192: {1.0: 0.901, 0.75: 0.889, 0.5: 0.854, 0.35: 0.812},\n",
    "                    160: {1.0: 0.890, 0.75: 0.873, 0.5: 0.832, 0.35: 0.791},\n",
    "                    128: {1.0: 0.869, 0.75: 0.855, 0.5: 0.808, 0.35: 0.750},\n",
    "                    96: {1.0: 0.832, 0.75: 0.816, 0.5: 0.758, 0.35: 0.704},\n",
    "                    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    #args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    print(args)\n",
    "    if args.seed is None:\n",
    "        args.seed = random.randint(1, 10000)\n",
    "    print(\"Random Seed: \", args.seed)\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.gpus:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    if args.evaluate:\n",
    "        args.results_dir = '/tmp'\n",
    "    if args.save == '':\n",
    "        args.save = time_stamp\n",
    "    save_path = os.path.join(args.results_dir, args.save)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    if args.gpus is not None:\n",
    "        args.gpus = [int(i) for i in args.gpus.split(',')]\n",
    "        device = 'cuda:' + str(args.gpus[0])\n",
    "        cudnn.benchmark = True\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "\n",
    "    if args.type == 'float64':\n",
    "        dtype = torch.float64\n",
    "    elif args.type == 'float32':\n",
    "        dtype = torch.float32\n",
    "    elif args.type == 'float16':\n",
    "        dtype = torch.float16\n",
    "    else:\n",
    "        raise ValueError('Wrong type!')  # TODO int8\n",
    "\n",
    "    model = MobileNet2(input_size=args.input_size, scale=args.scaling)\n",
    "    num_parameters = sum([l.nelement() for l in model.parameters()])\n",
    "    print(model)\n",
    "    print('number of parameters: {}'.format(num_parameters))\n",
    "    print('FLOPs: {}'.format(\n",
    "        count_flops(MobileNet2,\n",
    "                    args.batch_size // len(args.gpus) if args.gpus is not None else args.batch_size,\n",
    "                    device, dtype, args.input_size, 3, args.scaling)))\n",
    "\n",
    "    train_loader, val_loader = get_loaders(args.dataroot, args.batch_size, args.batch_size, args.input_size,\n",
    "                                           args.workers)\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if args.gpus is not None:\n",
    "        model = torch.nn.DataParallel(model, args.gpus)\n",
    "    model.to(device=device, dtype=dtype)\n",
    "    criterion.to(device=device, dtype=dtype)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=args.momentum, weight_decay=args.decay,\n",
    "                                nesterov=True)\n",
    "    if args.find_clr:\n",
    "        find_bounds_clr(model, train_loader, optimizer, criterion, device, dtype, min_lr=args.min_lr,\n",
    "                        max_lr=args.max_lr, step_size=args.epochs_per_step * len(train_loader), mode=args.mode,\n",
    "                        save_path=save_path)\n",
    "        return\n",
    "\n",
    "    if args.clr:\n",
    "        scheduler = CyclicLR(optimizer, base_lr=args.min_lr, max_lr=args.max_lr,\n",
    "                             step_size=args.epochs_per_step * len(train_loader), mode=args.mode)\n",
    "    else:\n",
    "        scheduler = MultiStepLR(optimizer, milestones=args.schedule, gamma=args.gamma)\n",
    "\n",
    "    best_test = 0\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    data = None\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume, map_location=device)\n",
    "            args.start_epoch = checkpoint['epoch'] - 1\n",
    "            best_test = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        elif os.path.isdir(args.resume):\n",
    "            checkpoint_path = os.path.join(args.resume, 'checkpoint.pth.tar')\n",
    "            csv_path = os.path.join(args.resume, 'results.csv')\n",
    "            print(\"=> loading checkpoint '{}'\".format(checkpoint_path))\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            args.start_epoch = checkpoint['epoch'] - 1\n",
    "            best_test = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint['epoch']))\n",
    "            data = []\n",
    "            with open(csv_path) as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    data.append(row)\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    if args.evaluate:\n",
    "        loss, top1, top5 = test(model, val_loader, criterion, device, dtype)  # TODO\n",
    "        return\n",
    "\n",
    "    csv_logger = CsvLogger(filepath=save_path, data=data)\n",
    "    csv_logger.save_params(sys.argv, args)\n",
    "\n",
    "    claimed_acc1 = None\n",
    "    claimed_acc5 = None\n",
    "    if args.input_size in claimed_acc_top1:\n",
    "        if args.scaling in claimed_acc_top1[args.input_size]:\n",
    "            claimed_acc1 = claimed_acc_top1[args.input_size][args.scaling]\n",
    "            claimed_acc5 = claimed_acc_top5[args.input_size][args.scaling]\n",
    "            csv_logger.write_text(\n",
    "                'Claimed accuracies are: {:.2f}% top-1, {:.2f}% top-5'.format(claimed_acc1 * 100., claimed_acc5 * 100.))\n",
    "    train_network(args.start_epoch, args.epochs, scheduler, model, train_loader, val_loader, optimizer, criterion,\n",
    "                  device, dtype, args.batch_size, args.log_interval, csv_logger, save_path, claimed_acc1, claimed_acc5,\n",
    "                  best_test)\n",
    "\n",
    "\n",
    "def train_network(start_epoch, epochs, scheduler, model, train_loader, val_loader, optimizer, criterion, device, dtype,\n",
    "                  batch_size, log_interval, csv_logger, save_path, claimed_acc1, claimed_acc5, best_test):\n",
    "    for epoch in trange(start_epoch, epochs + 1):\n",
    "        if not isinstance(scheduler, CyclicLR):\n",
    "            scheduler.step()\n",
    "        train_loss, train_accuracy1, train_accuracy5, = train(model, train_loader, epoch, optimizer, criterion, device,\n",
    "                                                              dtype, batch_size, log_interval, scheduler)\n",
    "        test_loss, test_accuracy1, test_accuracy5 = test(model, val_loader, criterion, device, dtype)\n",
    "        csv_logger.write({'epoch': epoch + 1, 'val_error1': 1 - test_accuracy1, 'val_error5': 1 - test_accuracy5,\n",
    "                          'val_loss': test_loss, 'train_error1': 1 - train_accuracy1,\n",
    "                          'train_error5': 1 - train_accuracy5, 'train_loss': train_loss})\n",
    "        save_checkpoint({'epoch': epoch + 1, 'state_dict': model.state_dict(), 'best_prec1': best_test,\n",
    "                         'optimizer': optimizer.state_dict()}, test_accuracy1 > best_test, filepath=save_path)\n",
    "\n",
    "        csv_logger.plot_progress(claimed_acc1=claimed_acc1, claimed_acc5=claimed_acc5)\n",
    "\n",
    "        if test_accuracy1 > best_test:\n",
    "            best_test = test_accuracy1\n",
    "\n",
    "    csv_logger.write_text('Best accuracy is {:.2f}% top-1'.format(best_test * 100.))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
